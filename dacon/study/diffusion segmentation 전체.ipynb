{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyOnMvIt+wzth5PpLa30QuT3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"DhOZ811VP50G"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import os\n","import cv2\n","from PIL import Image\n","import pandas as pd\n","\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","\n","from tqdm import tqdm\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Diffusion process parameters\n","T = 100  # Number of diffusion steps\n","beta_min = 0.01  # Minimum value of beta (for noise)\n","\n","# Create a diffusion process schedule\n","betas = np.linspace(0, beta_min, T)\n","betas = torch.tensor(betas, dtype=torch.float32)\n","\n","# Define a simple generator and discriminator network\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.fc = nn.Linear(64, 784)  # Example: Mapping from noise vector to image space\n","\n","    def forward(self, z, t):\n","        noise = torch.randn_like(z)\n","        x = (1 - t.view(-1, 1, 1, 1)) * self.fc(z) + t.view(-1, 1, 1, 1) * noise\n","        return x\n","\n","# Define a simple discriminator network (example)\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(784, 128),  # Example: Mapping from image space to a binary classification\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, x):\n","        return self.fc(x)\n","\n","# Define the U-Net architecture for segmentation\n","class UNet(nn.Module):\n","    # U-Net architecture definition (same as before)\n","    def __init__(self, in_channels, out_channels):\n","        super(UNet, self).__init__()\n","        # Encoder\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # Decoder\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2)\n","        )\n","\n","    def forward(self, x):\n","        x1 = self.encoder(x)\n","        x2 = self.decoder(x1)\n","        return x2\n","\n","# Initialize the U-Net segmentation model\n","\n","num_classes = 10  # 예시: 클래스 수에 맞게 설정\n","\n","in_channels = 3  # Input channels (e.g., for RGB images)\n","out_channels = num_classes  # Number of segmentation classes\n","segmentation_model = UNet(in_channels, out_channels)\n","\n","# Define segmentation loss function (e.g., cross-entropy loss)\n","segmentation_criterion = nn.CrossEntropyLoss()\n","\n","# Define optimizer for segmentation model\n","segmentation_optimizer = torch.optim.Adam(segmentation_model.parameters(), lr=0.0001)\n","\n","# Define the reverse diffusion loss (MSE loss)\n","reverse_diffusion_criterion = nn.MSELoss()\n","\n","# Training loop\n","# Training parameters\n","num_epochs = 100  # 적절한 에폭 수로 설정\n","print_interval = 10  # 손실 출력 간격 설정\n","\n","# Training loop\n","for epoch in range(num_epochs):\n","    total_loss = 0.0  # 손실을 누적할 변수 초기화\n","\n","    for step, (real_images, labels) in enumerate(data_loader):\n","        real_images = real_images.view(real_images.size(0), -1)\n","        batch_size = real_images.size(0)\n","        z = torch.randn(batch_size, 64)\n","\n","        for t in range(T):\n","            current_beta = betas[t]\n","\n","            # Forward Diffusion process: Add noise to the initial image\n","            noise = torch.randn_like(real_images) * (current_beta ** 0.5)\n","            x_t = real_images + noise\n","\n","            # Segmentation\n","            segmentation_output = segmentation_model(x_t)\n","\n","            # Calculate segmentation loss\n","            segmentation_loss = segmentation_criterion(segmentation_output, labels)\n","\n","            # Update segmentation model\n","            segmentation_optimizer.zero_grad()\n","            segmentation_loss.backward()\n","            segmentation_optimizer.step()\n","\n","            # Reverse Diffusion process: Remove noise from the image\n","            noise = torch.randn_like(x_t) * (current_beta ** 0.5)\n","            x_t = x_t - noise\n","\n","            # Calculate Reverse Diffusion loss (MSE loss)\n","            reverse_diffusion_loss = reverse_diffusion_criterion(x_t, real_images)\n","\n","            # Update the segmentation model using the reverse diffusion optimizer\n","            segmentation_optimizer.zero_grad()  # Clear gradients from segmentation model\n","            reverse_diffusion_loss.backward()\n","            segmentation_optimizer.step()  # Update segmentation model parameters\n","\n","        # 손실을 누적\n","        total_loss += segmentation_loss.item() + reverse_diffusion_loss.item()\n","\n","    # 평균 손실 계산\n","    average_loss = total_loss / len(data_loader)\n","\n","    # 일정 간격으로 손실 출력\n","    if (epoch + 1) % print_interval == 0:\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {average_loss:.4f}\")\n","\n","# 학습 완료 후에도 손실 출력\n","print(f\"Training completed. Final loss: {average_loss:.4f}\")\n","\n","# Inference loop for segmentation\n","segmentation_results = []\n","for _ in range(num_samples):\n","    z = torch.randn(1, 64)\n","    x_t = torch.zeros(1, 3, 128, 128)  # Adjust dimensions and channels as needed\n","    for t in range(T):\n","        current_beta = betas[t]\n","        noise = torch.randn_like(x_t) * (current_beta ** 0.5)\n","        x_t = x_t + noise\n","    # Perform segmentation on x_t using the trained segmentation model\n","    segmentation_map = segmentation_model(x_t)\n","    segmentation_results.append(segmentation_map)\n","\n","# Visualize or analyze segmentation results\n","\n","# 시각화 함수 추가\n","def visualize_segmentation(image, segmentation_map, title=\"Segmentation\"):\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    ax1.imshow(image)\n","    ax1.set_title(\"Input Image\")\n","\n","    ax2.imshow(segmentation_map, cmap=\"viridis\")  # 적절한 컬러맵 사용\n","    ax2.set_title(title)\n","\n","    plt.show()\n","\n","# 결과 시각화\n","for i, segmentation_map in enumerate(segmentation_results):\n","    input_image = data_loader[i][0]  # 입력 이미지\n","    visualize_segmentation(input_image, segmentation_map, title=f\"Segmentation Result {i+1}\")"]}]}