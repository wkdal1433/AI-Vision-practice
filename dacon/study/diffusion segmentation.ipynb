{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyNxF4EjUbaidapq8caleQ/5"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# 필요한 라이브러리를 임포트합니다.\n","import os  # 파일 및 디렉토리 작업을 위한 Python 내장 라이브러리\n","import cv2  # OpenCV 라이브러리를 사용하여 이미지 처리 작업을 수행\n","from PIL import Image  # 이미지를 열고 처리하기 위한 Pillow 라이브러리\n","import pandas as pd  # 데이터 분석 및 조작을 위한 라이브러리\n","import numpy as np  # 과학적 계산을 위한 다차원 배열 라이브러리\n","\n","# PyTorch 및 관련 모듈을 임포트합니다.\n","import torch  # 딥러닝 모델을 구축하고 학습하기 위한 기본 라이브러리\n","import torch.nn as nn  # PyTorch의 신경망 모듈을 포함한 하위 모듈\n","from torch.utils.data import Dataset, DataLoader  # 데이터 로딩 및 처리 도구\n","from torchvision import transforms  # 이미지 데이터 전처리 및 변환 모듈\n","import torchvision\n","\n","# 데이터 증강 및 전처리를 위한 Albumentations 라이브러리를 임포트합니다.\n","from tqdm import tqdm  # 진행률 표시를 위한 라이브러리\n","import albumentations as A  # 이미지 데이터 증강을 위한 라이브러리\n","from albumentations.pytorch import ToTensorV2  # Albumentations 출력을 PyTorch 텐서로 변환\n","\n","\n","# 시각화\n","import matplotlib.pyplot as plt\n","\n","# 사용 가능한 GPU가 있다면 'cuda'를, 그렇지 않으면 'cpu'를 선택하여 디바이스를 설정합니다.\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"],"metadata":{"id":"j412hYiNESlL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 코드는 확산 프로세스의 파라미터 및 스케줄을 설정하는 부분으로, 시간에 따른 베타 값을 선형적으로 변화시켜 확산 프로세스를 모델링합니다. 이 프로세스는 후속 딥러닝 모델에 사용될 수 있으며, 시간에 따라 노이즈를 적용하는 데 사용됩니다. 주어진 주석을 통해 코드의 의미와 목적을 이해하실 수 있을 것입니다."],"metadata":{"id":"fZd9CBoDEc9L"}},{"cell_type":"code","source":["# Diffusion process parameters\n","T = 100  # 확산 프로세스 단계 수, 시간의 흐름을 모델링하는 데 사용됨\n","beta_min = 0.01  # 베타의 최소값 (노이즈에 대한 최소 가중치)\n","\n","# Create a diffusion process schedule\n","# 확산 프로세스 스케줄 생성\n","\n","# 베타 값을 시간에 따라 선형적으로 변화시킵니다.\n","# 초기 값은 0이며 T 단계 동안 beta_min 값에 수렴합니다.\n","# 이러한 베타 값은 확률적 모델에서 사용될 수 있습니다.\n","betas = np.linspace(0, beta_min, T)\n","\n","# NumPy 배열을 PyTorch 텐서로 변환합니다.\n","betas = torch.tensor(betas, dtype=torch.float32)"],"metadata":{"id":"CtHlLnGiEcSG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 코드는 간단한 생성자와 판별자 신경망을 정의합니다. 생성자는 잡음 벡터를 입력으로 받고, 시간 변수 t에 따라 잡음과 생성된 이미지를 혼합하여 결과를 생성합니다. 판별자는 이미지를 입력으로 받고, 이미지가 진짜인지 가짜인지 이진 분류 결과를 반환합니다. 이러한 생성자와 판별자는 확률적 모델에서 사용되며, 주로 이미지 생성 GANs (Generative Adversarial Networks)에서 사용됩니다. 코드 주석을 통해 각 부분의 역할과 목적을 이해하실 수 있을 것입니다. 다른 질문이나 추가 설명이 필요하면 언제든지 물어보세요.\n","\n","\n","\n","\n","\n"],"metadata":{"id":"3IqCW6oZFuhn"}},{"cell_type":"code","source":["# Define a simple generator and discriminator network\n","\n","# 간단한 생성자 (Generator) 신경망 정의\n","class Generator(nn.Module):\n","    def __init__(self):\n","        super(Generator, self).__init__()\n","        self.fc = nn.Linear(64, 784)  # 예시: 잡음 벡터를 이미지 공간으로 매핑\n","\n","    def forward(self, z, t):\n","        # 노이즈를 생성하고, 시간 t에 따라 노이즈와 생성된 이미지를 혼합\n","        noise = torch.randn_like(z)  # 잡음 생성\n","        x = (1 - t.view(-1, 1, 1, 1)) * self.fc(z) + t.view(-1, 1, 1, 1) * noise\n","        return x\n","\n","# 간단한 판별자 (Discriminator) 신경망 정의 (예시)\n","class Discriminator(nn.Module):\n","    def __init__(self):\n","        super(Discriminator, self).__init__()\n","        self.fc = nn.Sequential(\n","            nn.Linear(784, 128),  # 예시: 이미지 공간에서 이진 분류를 위한 매핑\n","            nn.ReLU(),\n","            nn.Linear(128, 1)\n","        )\n","\n","    def forward(self, x):\n","        # 입력 이미지를 판별하여 이진 분류 결과 반환\n","        return self.fc(x)"],"metadata":{"id":"xN9uN6HbEmbd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 코드는 세그멘테이션 작업을 위한 U-Net 아키텍처를 정의합니다. U-Net은 인코더와 디코더로 구성되며, 주로 이미지 세그멘테이션 작업에서 사용됩니다. 인코더는 입력 이미지를 다운샘플링하고 특징을 추출하는 역할을 하며, 디코더는 이러한 특징을 업샘플링하여 최종 세그멘테이션 맵을 생성합니다. 주석을 통해 각 부분의 역할과 목적을 이해하실 수 있을 것입니다. 다른 질문이나 추가 설명이 필요하면 언제든지 물어보세요."],"metadata":{"id":"Gx_obUwtF69K"}},{"cell_type":"code","source":["# Define the U-Net architecture for segmentation\n","\n","# 세그멘테이션을 위한 U-Net 아키텍처 정의\n","class UNet(nn.Module):\n","    # U-Net 아키텍처 정의 (이전과 동일)\n","    def __init__(self, in_channels, out_channels):\n","        super(UNet, self).__init__()\n","        # 인코더 (Encoder) 부분 정의\n","        self.encoder = nn.Sequential(\n","            nn.Conv2d(in_channels, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.MaxPool2d(kernel_size=2, stride=2)\n","        )\n","\n","        # 디코더 (Decoder) 부분 정의\n","        self.decoder = nn.Sequential(\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.ConvTranspose2d(64, out_channels, kernel_size=2, stride=2)\n","        )\n","\n","    def forward(self, x):\n","        # 인코더 부분을 통과한 후 디코더 부분을 통과시켜 세그멘테이션 결과를 반환\n","        x1 = self.encoder(x)\n","        x2 = self.decoder(x1)\n","        return x2"],"metadata":{"id":"DMV7cX6DEpmN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 코드는 U-Net 세그멘테이션 모델을 초기화하고, 세그멘테이션 작업을 위한 손실 함수와 옵티마이저를 정의하는 부분입니다. U-Net은 이미지 세그멘테이션 작업에서 자주 사용되는 신경망 아키텍처 중 하나이며, Cross-Entropy 손실 함수는 세그멘테이션 작업에 적합한 손실 함수 중 하나입니다. Adam 옵티마이저를 사용하여 모델을 최적화하고, 역확산 손실 함수로는 MSE 손실 함수를 사용하고 있습니다."],"metadata":{"id":"fdZtyEZiGHna"}},{"cell_type":"code","source":["# Initialize the U-Net segmentation model\n","\n","# 예시: 클래스 수에 맞게 설정된 변수\n","num_classes = 10\n","\n","# Input 이미지의 채널 수 (예: RGB 이미지의 경우 3 채널)\n","in_channels = 3\n","\n","# 세그멘테이션 클래스의 수를 지정하는 변수\n","out_channels = num_classes\n","\n","# U-Net 세그멘테이션 모델을 초기화합니다.\n","segmentation_model = UNet(in_channels, out_channels)\n","\n","# Define segmentation loss function (e.g., cross-entropy loss)\n","\n","# 세그멘테이션 손실 함수를 정의합니다. 주로 Cross-Entropy 손실 함수를 사용합니다.\n","segmentation_criterion = nn.CrossEntropyLoss()\n","\n","# Define optimizer for segmentation model\n","\n","# 세그멘테이션 모델을 최적화하기 위한 Adam 옵티마이저를 정의합니다.\n","# 학습률 (lr)은 0.0001로 설정되어 있습니다.\n","segmentation_optimizer = torch.optim.Adam(segmentation_model.parameters(), lr=0.0001)\n","\n","# Define the reverse diffusion loss (MSE loss)\n","\n","# 역확산 손실 함수를 정의합니다. 평균 제곱 오차 (MSE) 손실 함수를 사용합니다.\n","reverse_diffusion_criterion = nn.MSELoss()"],"metadata":{"id":"5-dt1LZbEvWe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 코드는 U-Net 세그멘테이션 모델의 학습을 수행하는 루프입니다. 각 에폭(epoch)마다 데이터로더(data_loader)에서 미니배치를 가져와 U-Net 모델을 학습합니다. 확산(diffusion) 과정과 역확산(reverse diffusion) 과정이 번갈아 가면서 진행되며, 각각의 과정에서 손실을 계산하고 모델을 업데이트합니다. 학습이 완료된 후에는 최종 손실을 출력합니다."],"metadata":{"id":"9Y_T7bcpGTWe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"gU60A1RfDQXC"},"outputs":[],"source":["# Training loop\n","for epoch in range(num_epochs):\n","    total_loss = 0.0  # 손실을 누적할 변수 초기화\n","\n","    for step, (real_images, labels) in enumerate(data_loader):\n","        real_images = real_images.view(real_images.size(0), -1)\n","        batch_size = real_images.size(0)\n","        z = torch.randn(batch_size, 64)\n","\n","        for t in range(T):\n","            current_beta = betas[t]\n","\n","            # Forward Diffusion process: Add noise to the initial image\n","\n","            # 초기 이미지에 노이즈를 추가하여 확산 과정 수행\n","            noise = torch.randn_like(real_images) * (current_beta ** 0.5)\n","            x_t = real_images + noise\n","\n","            # Segmentation\n","            segmentation_output = segmentation_model(x_t)\n","\n","            # Calculate segmentation loss\n","\n","            # 세그멘테이션 모델의 출력과 실제 레이블 간의 손실 계산\n","            segmentation_loss = segmentation_criterion(segmentation_output, labels)\n","\n","            # Update segmentation model\n","\n","            # 세그멘테이션 모델의 가중치 업데이트\n","            segmentation_optimizer.zero_grad()\n","            segmentation_loss.backward()\n","            segmentation_optimizer.step()\n","\n","            # Reverse Diffusion process: Remove noise from the image\n","\n","            # 확산 과정을 역으로 수행하여 이미지에서 노이즈 제거\n","            noise = torch.randn_like(x_t) * (current_beta ** 0.5)\n","            x_t = x_t - noise\n","\n","            # Calculate Reverse Diffusion loss (MSE loss)\n","\n","            # 역확산 손실 계산 (평균 제곱 오차 손실)\n","            reverse_diffusion_loss = reverse_diffusion_criterion(x_t, real_images)\n","\n","            # Update the segmentation model using the reverse diffusion optimizer\n","\n","            # 역확산 손실을 사용하여 세그멘테이션 모델 업데이트\n","            segmentation_optimizer.zero_grad()  # 세그멘테이션 모델의 그래디언트 초기화\n","            reverse_diffusion_loss.backward()\n","            segmentation_optimizer.step()  # 세그멘테이션 모델의 파라미터 업데이트\n","\n","        # 손실을 누적\n","        total_loss += segmentation_loss.item() + reverse_diffusion_loss.item()\n","\n","    # 평균 손실 계산\n","\n","    # 현재 에폭의 평균 손실 계산\n","    average_loss = total_loss / len(data_loader)\n","\n","    # 일정 간격으로 손실 출력\n","\n","    # 지정한 간격마다 현재 에폭의 평균 손실 출력\n","    if (epoch + 1) % print_interval == 0:\n","        print(f\"Epoch [{epoch + 1}/{num_epochs}] Loss: {average_loss:.4f}\")\n","\n","# 학습 완료 후에도 손실 출력\n","\n","# 학습이 완료된 후 최종 평균 손실 출력\n","print(f\"Training completed. Final loss: {average_loss:.4f}\")"]},{"cell_type":"markdown","source":["이 코드는 세그멘테이션 모델을 사용하여 이미지 세그멘테이션을 수행하는 추론 루프를 나타냅니다. 주석을 통해 다음과 같은 추론 프로세스를 설명하고 있습니다:\n","\n","바깥쪽 루프 (for _ in range(num_samples))는 추론을 여러 번 수행하는 데 사용됩니다. num_samples는 추론 반복 횟수를 나타냅니다.\n","\n","잡음 벡터 z를 생성합니다. 이 벡터는 추론의 초기 입력으로 사용됩니다.\n","\n","초기 입력 이미지 x_t를 생성합니다. 이 예시에서는 크기가 128x128이고 3개의 채널을 가지는 이미지를 사용합니다. 필요에 따라 이미지의 크기와 채널 수를 조절할 수 있습니다.\n","\n","시간 스텝 T 동안 다음을 반복합니다:\n","\n","현재 시간 스텝에 해당하는 베타 값(current_beta)을 가져옵니다.\n","초기 이미지에 노이즈를 추가한 후(x_t), 훈련된 세그멘테이션 모델을 사용하여 세그멘테이션 맵을 생성합니다.\n","생성된 세그멘테이션 맵을 segmentation_results 리스트에 추가합니다.\n","\n","이 코드는 학습된 세그멘테이션 모델을 사용하여 이미지에 대한 세그멘테이션을 추론하고, 그 결과를 리스트에 저장하는 추론 프로세스를 나타냅니다."],"metadata":{"id":"GXJOmwpcGe8X"}},{"cell_type":"code","source":["# Inference loop for segmentation (세그멘테이션 추론 루프)\n","segmentation_results = []  # 세그멘테이션 결과를 저장할 리스트\n","\n","for _ in range(num_samples):\n","    z = torch.randn(1, 64)  # 잡음 벡터 생성\n","\n","    # 초기 입력 이미지를 생성 (크기와 채널 수를 필요에 따라 조절)\n","    x_t = torch.zeros(1, 3, 128, 128)  # 예시에서는 3개의 채널과 128x128 크기의 이미지를 사용\n","\n","    for t in range(T):\n","        current_beta = betas[t]  # 현재 시간 스텝에 해당하는 베타 값\n","\n","        # 확산 프로세스: 초기 이미지에 노이즈 추가\n","        noise = torch.randn_like(x_t) * (current_beta ** 0.5)\n","        x_t = x_t + noise\n","\n","    # 훈련된 세그멘테이션 모델을 사용하여 x_t에 대한 세그멘테이션 수행\n","    segmentation_map = segmentation_model(x_t)\n","\n","    # 세그멘테이션 결과를 리스트에 추가\n","    segmentation_results.append(segmentation_map)"],"metadata":{"id":"Sg3GMIS9EyHg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualize or analyze segmentation results\n","\n","# 시각화 함수 추가\n","def visualize_segmentation(image, segmentation_map, title=\"Segmentation\"):\n","    # 입력 이미지와 세그멘테이션 결과를 시각화하는 함수 정의\n","    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n","\n","    ax1.imshow(image)\n","    ax1.set_title(\"Input Image\")\n","\n","    ax2.imshow(segmentation_map, cmap=\"viridis\")  # 적절한 컬러맵 사용\n","    ax2.set_title(title)\n","\n","    plt.show()\n","\n","# 결과 시각화\n","for i, segmentation_map in enumerate(segmentation_results):\n","    # 세그멘테이션 결과를 시각화하는 반복문\n","    input_image = data_loader[i][0]  # 입력 이미지를 가져옴\n","    visualize_segmentation(input_image, segmentation_map, title=f\"Segmentation Result {i+1}\")"],"metadata":{"id":"mjzse0eTE173"},"execution_count":null,"outputs":[]}]}