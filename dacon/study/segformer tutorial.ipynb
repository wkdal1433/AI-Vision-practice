{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyO7xELTj1G6oHiyknABc+Ks"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NZyUuzNvdHvj"},"outputs":[],"source":["pip install -r requirements.txt\n"]},{"cell_type":"code","source":["python ./utils/download_dataset.py --savedir=\"./dataset/camvid/\"\n"],"metadata":{"id":"R8KYp-DBdL3-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class CamvidDataset(Dataset):\n","\n","  def __init__(self,\n","               root_dir,\n","               image_filenames,\n","               masks_filenames,\n","               feature_extractor,\n","               augment=False,\n","               num_classes=12) -> None:\n","\n","    self.root_dir = root_dir\n","    self.image_filenames = image_filenames\n","    self.masks_filenames = masks_filenames\n","    self.num_classes = num_classes\n","    self.feature_extractor = feature_extractor\n","\n","\n","    conf_file = os.path.join(root_dir,'label_colors11.txt')\n","\n","    colors, labels = self._dataset_conf(conf_file)\n","    self.id2label = dict(zip(range(self.num_classes),labels))\n","    self.class_colors = labelColors\n"],"metadata":{"id":"3NeJOraFdMzu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def __len__(self):\n","    return len(self.image_filenames)\n"],"metadata":{"id":"hK_ikARSdOYF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def __getitem__(self,idx):\n","\n","    image_filename = self.image_filenames[idx]\n","    mask_filename = self.masks_filenames[idx]\n","\n","    image = cv2.imread(os.path.join(self.root_dir,image_filename),)# BGR image\n","    mask = cv2.imread(\n","            os.path.join(self.root_dir,mask_filename),\n","            cv2.IMREAD_UNCHANGED,\n","        ) # BGR image\n","\n","    # convert the mask from bgr to grayscale\n","    mask = self.bgr2gray12(mask,self.class_colors)\n","\n","    if self.augment :\n","      image, mask = self._data_augmentation(image,mask)\n","\n","    encod_inputs =self.feature_extractor(image,mask, return_tensors='pt')\n","\n","    for k,v in encod_inputs.items():\n","      encod_inputs[k].squeeze_()\n","\n","    return encod_inputs\n"],"metadata":{"id":"O-LQKBZkdPRq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def _data_augmentation(self, image, mask):\n","    aug = A.Compose(\n","      [\n","          A.Flip(p=0.5),\n","          A.RandomRotate90(p=0.5),\n","          A.OneOf([\n","                  A.Affine(p=0.33,shear=(-5,5),rotate=(-80,90)),\n","                  A.ShiftScaleRotate(\n","                    shift_limit=0.2,\n","                    scale_limit=0.2,\n","                    rotate_limit=120,\n","                    #border_mode= cv2.BORDER_CONSTANT,\n","                    #value=255, # padding with the ignored class\n","                    p=0.33),\n","                  A.GridDistortion(p=0.33),\n","                ], p=1),\n","          A.CLAHE(p=0.8),\n","          A.OneOf(\n","              [\n","                  A.ColorJitter(p=0.33),\n","                  A.RandomBrightnessContrast(p=0.33),\n","                  A.RandomGamma(p=0.33)\n","              ],\n","              p=1\n","          )\n","          ]\n","    )\n","    augmentation = aug(image=image, mask=mask)\n","    aug_img, aug_mask = augmentation['image'], augmentation['mask']\n","    return aug_img, aug_mask\n"],"metadata":{"id":"pTQbWA-ddQii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Returns a non batched dataset\n","def get_dataset(data_path='/dataset/camvid/',\n","                val_split=0.2,\n","                random_state=42,\n","                feature_extractor_name='nvidia/segformer-b2-finetuned-cityscapes-1024-1024'):\n","\n","  feature_extractor = SegformerImageProcessor.from_pretrained(feature_extractor_name)\n","  feature_extractor.do_reduce_labels = False\n","  feature_extractor.do_resize = True\n","  feature_extractor.size = {\"height\":360, \"width\":480}\n","  feature_extractor.do_normalize= False\n","  feature_extractor.do_rescale= True\n","\n","\n","  img_files, mask_files = get_data_filenames(data_path)\n","\n","  train_imgs, val_imgs, train_masks, val_masks = train_test_split(\n","      img_files, mask_files, test_size=val_split, random_state=random_state, shuffle=True)\n","\n","  train_dataset = CamvidDataset(data_path,\n","                                train_imgs, train_masks,\n","                                feature_extractor,\n","                                augment=True,\n","                                num_classes=12\n","                                )\n","  val_dataset = CamvidDataset(data_path,\n","                              val_imgs,\n","                              val_masks,\n","                              feature_extractor,\n","                              num_classes=12)\n","  return train_dataset, val_dataset\n"],"metadata":{"id":"MoNMm8g4dRlh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# counts number of samples in each class\n","def compute_class_distribution(dataset):\n","  summary = [0]*dataset.num_classes\n","\n","  for inputs in dataset:\n","      mask = inputs['labels']\n","      labels, counts = np.unique(mask, return_counts=True)\n","      for idx,label in enumerate(labels):\n","        summary[label] += counts[idx]\n","  return summary\n","\n","# computes the weight of each class\n","def compute_class_weights(total, class_counts):\n","  weights = []\n","  for class_count in class_counts:\n","    weights.append(total/class_count)\n","  return weights\n"],"metadata":{"id":"rEV99SUDdSoT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Returns batched dataset\n","def get_dataloader(dataset,\n","                   train_batch_size=10,\n","                   val_batch_size=7,\n","                   num_workers=2,\n","                   prefetch_factor=5):\n","\n","\n","  train_dataset, val_dataset = dataset[0], dataset[1]\n","\n","  train_dataloader = DataLoader(train_dataset,\n","                                batch_size=train_batch_size,\n","                                shuffle=True,\n","                                num_workers=num_workers,\n","                                prefetch_factor=prefetch_factor)\n","\n","  val_dataloader = DataLoader(val_dataset,\n","                              batch_size=val_batch_size,\n","                              num_workers=num_workers,\n","                              prefetch_factor=prefetch_factor)\n","\n","  return train_dataloader, val_dataloader\n"],"metadata":{"id":"wPqNnWI-dToD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class SegFormerFineTuned(pl.LightningModule):\n","  def __init__(self, id2label,\n","               train_dl,\n","               val_dl,\n","               metrics_interval,\n","               class_weights,\n","               model_path=\"nvidia/segformer-b2-finetuned-cityscapes-1024-1024\"):\n","\n","    super(SegFormerFineTuned, self).__init__()\n","    self.id2label = id2label\n","    self.metrics_interval = metrics_interval\n","    self.train_dl = train_dl\n","    self.val_dl = val_dl\n","    self.weights = class_weights\n","    self.model_path = model_path\n","\n","    self.num_classes = len(id2label.keys())\n","    self.label2id = {v:k for k,v in self.id2label.items()}\n","\n","    self.model = SegformerForSemanticSegmentation.from_pretrained(\n","        self.model_path,\n","        return_dict=False,\n","        num_labels=self.num_classes,\n","        id2label=self.id2label,\n","        label2id=self.label2id,\n","        ignore_mismatched_sizes=True,\n","    )\n","\n","    self.train_mean_iou = evaluate.load(\"mean_iou\")\n","    self.val_mean_iou = evaluate.load(\"mean_iou\")\n","    self.test_mean_iou = evaluate.load(\"mean_iou\")\n","\n","    # Save the hyper-parameters\n","    # with the checkpoints\n","    self.save_hyperparameters()\n","\n","  def forward(self, images, masks):\n","    outputs = self.model(pixel_values=images)\n","    return (outputs)\n","\n","  def training_step(self, batch, num_batch):\n","    images, masks = batch['pixel_values'], batch['labels']\n","\n","    # Forward pass\n","\n","    predictions = self(images,masks)[0]\n","\n","    # upsample the predictions\n","    # from size (H/4,W/4) -> (H,W)\n","    predictions = torch.nn.functional.interpolate(\n","            predictions,\n","            size=masks.shape[-2:],\n","            mode=\"nearest-exact\",\n","            align_corners=False\n","        )\n","\n","    weighted_loss = CrossEntropyLoss(weight=self.weights,ignore_index=255)\n","    loss = weighted_loss(predictions,masks)\n","\n","    predictions = predictions.argmax(dim=1)\n","\n","\n","    # Evaluate the model\n","    self.train_mean_iou.add_batch(\n","            predictions= predictions.detach().cpu().numpy(),\n","            references=masks.detach().cpu().numpy()\n","        )\n","    if num_batch % self.metrics_interval == 0:\n","\n","        metrics = self.train_mean_iou.compute(\n","            num_labels=self.num_classes,\n","            ignore_index=255,\n","            reduce_labels=False,\n","        )\n","\n","        metrics = {'loss': loss, \"mean_iou\": metrics[\"mean_iou\"], \"mean_accuracy\": metrics[\"mean_accuracy\"]}\n","\n","        for k,v in metrics.items():\n","            self.log(k,v)\n","\n","        return(metrics)\n","    else:\n","        return({'loss': loss})\n","\n","  def validation_step(self, batch, num_batch):\n","    images, masks = batch['pixel_values'], batch['labels']\n","\n","    # Forward pass\n","\n","    predictions = self(images,masks)[0]\n","\n","    # up-samples the predictions\n","    # from size (H/4,W/4) -> (H,W)\n","    predictions = torch.nn.functional.interpolate(\n","            predictions,\n","            size=masks.shape[-2:],\n","            mode=\"nearest-exact\",\n","            align_corners=False\n","        )\n","    weighted_loss = CrossEntropyLoss(weight=self.weights,ignore_index=255)\n","    loss = weighted_loss(predictions,masks)\n","    predictions = predictions.argmax(dim=1)\n","\n","\n","    # Evaluate the model\n","    self.val_mean_iou.add_batch(\n","            predictions= predictions.detach().cpu().numpy(),\n","            references=masks.detach().cpu().numpy()\n","        )\n","\n","    return({'val_loss': loss})\n","\n","  def validation_epoch_end(self,outputs):\n","    metrics = self.val_mean_iou.compute(\n","              num_labels=self.num_classes,\n","              ignore_index=255,\n","              reduce_labels=False,\n","          )\n","\n","    avg_val_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n","    val_mean_iou = metrics[\"mean_iou\"]\n","    val_mean_accuracy = metrics[\"mean_accuracy\"]\n","\n","    metrics = {\"val_loss\": avg_val_loss, \"val_mean_iou\":val_mean_iou, \"val_mean_accuracy\":val_mean_accuracy}\n","    for k,v in metrics.items():\n","        self.log(k,v)\n","\n","    return metrics\n","\n","  def configure_optimizers(self):\n","    return torch.optim.Adam([p for p in self.parameters() if p.requires_grad], lr=2e-05, eps=1e-08)\n","\n","  def train_dataloader(self):\n","    return self.train_dl\n","\n","  def val_dataloader(self):\n","    return self.val_dl\n","\n"],"metadata":{"id":"qzkLzsT8dVH7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train_model(train_dataloader,\n","                val_dataloader,\n","                class_weights,\n","                id2label,\n","                hf_model_name=\"nvidia/segformer-b2-finetuned-cityscapes-1024-1024\",\n","                ckpt_path='/checkpoints/',\n","                accelerator_mode='gpu',\n","                devices=1,\n","                max_epochs=300,\n","                log_every_n_steps=8,\n","                last_ckpt_path=None,\n","                resume=False\n","            ):\n","\n","    if accelerator_mode == \"gpu\":\n","        model = SegFormerFineTuned(\n","            id2label,\n","            train_dl=train_dataloader,\n","            val_dl=val_dataloader,\n","            metrics_interval=log_every_n_steps,\n","            class_weights=torch.Tensor(class_weights).cuda(),\n","            model_path=hf_model_name\n","        )\n","    else:\n","        model = SegFormerFineTuned(\n","            id2label,\n","            train_dl=train_dataloader,\n","            val_dl=val_dataloader,\n","            metrics_interval=log_every_n_steps,\n","            class_weights=torch.Tensor(class_weights),\n","            model_path=hf_model_name\n","        )\n","\n","    # Callback to stop when the model stops improving\n","    early_stop_callback = EarlyStopping(\n","        monitor=\"val_loss\",\n","        min_delta=0.00,\n","        patience=3,\n","        verbose=False,\n","        mode=\"min\",\n","    )\n","    # monitor the evolution of training and validation metrics\n","    checkpoint_callback = ModelCheckpoint(save_top_k=1, monitor=\"val_loss\")\n","\n","    # Callback to see a prediction sample by the end of the training\n","    #visualize_callback = VisualizeSampleCallback()\n","\n","    trainer = pl.Trainer(\n","        default_root_dir=ckpt_path,\n","        accelerator=accelerator_mode,\n","        devices=devices,\n","        callbacks=[early_stop_callback, checkpoint_callback],\n","        max_epochs=max_epochs,\n","        log_every_n_steps= log_every_n_steps,\n","        val_check_interval=len(train_dataloader),\n","    )\n","\n","    if resume and last_ckpt_path:\n","      trainer.fit(model,ckpt_path=last_ckpt_path)\n","    else:\n","      trainer.fit(model)\n","\n","    return trainer, model\n"],"metadata":{"id":"7blMynDwdXRO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["res = trainer.validate(ckpt_path=\"best\")\n"],"metadata":{"id":"EIbWMFLAdZAL"},"execution_count":null,"outputs":[]}]}