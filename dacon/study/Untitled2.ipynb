{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"authorship_tag":"ABX9TyM7XJXdDru/YEmI3bJsgfa3"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"duI8f0kBUPHF"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["cd /content/drive/MyDrive/open"],"metadata":{"id":"8P3oV8-JVNlu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","from torchvision import transforms\n","import os\n","import cv2\n","from PIL import Image\n","import pandas as pd\n","import numpy as np"],"metadata":{"id":"-BbXC_1gVmEd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# RLE 인코딩 함수\n","# RLE(Run-Length Encoding)은 연속적으로 반복되는 값을 압축하는 방법 중 하나입니다.\n","\n","def rle_encode(mask):\n","    # 입력된 이진 마스크를 1차원 배열로 변환합니다.\n","    pixels = mask.flatten()\n","\n","    # 배열의 시작과 끝에 0을 추가하여 RLE 알고리즘을 적용하기 위한 기본 준비를 합니다.\n","    pixels = np.concatenate([[0], pixels, [0]])\n","\n","    # 연속하지 않는 값의 인덱스를 찾아내어 압축합니다.\n","    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n","\n","    # 압축된 런 길이를 계산합니다.\n","    runs[1::2] -= runs[::2]\n","\n","    # 압축된 런을 문자열로 변환하여 반환합니다.\n","    return ' '.join(str(x) for x in runs)"],"metadata":{"id":"XE6SnzNfVgzA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 사용자 정의 데이터셋 클래스\n","class CustomDataset(Dataset):\n","    def __init__(self, csv_file, transform=None, infer=False):\n","        # 주어진 CSV 파일을 읽어 데이터를 로드합니다.\n","        self.data = pd.read_csv(csv_file)\n","\n","        # 데이터 변환(transform) 및 추론 모드 여부(infer)를 설정합니다.\n","        self.transform = transform\n","        self.infer = infer\n","\n","    def __len__(self):\n","        # 데이터셋의 길이를 반환합니다.\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        # 주어진 인덱스(idx)를 사용하여 이미지 및 마스크 파일 경로를 가져옵니다.\n","        img_path = self.data.iloc[idx, 1]\n","        image = cv2.imread(img_path)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n","\n","        if self.infer:\n","            # 추론 모드(infer)인 경우, 이미지 변환(transform)을 적용하고 이미지만 반환합니다.\n","            if self.transform:\n","                image = self.transform(image=image)['image']\n","            return image\n","\n","        mask_path = self.data.iloc[idx, 2]\n","        mask = cv2.imread(mask_path, cv2.IMREAD_GRAYSCALE)\n","\n","        # 마스크 이미지에서 픽셀 값이 255인 부분을 12로 간주하여 변경합니다.\n","        mask[mask == 255] = 12\n","\n","        if self.transform:\n","            # 훈련 또는 검증 모드인 경우, 이미지와 마스크에 변환을 적용합니다.\n","            augmented = self.transform(image=image, mask=mask)\n","            image = augmented['image']\n","            mask = augmented['mask']\n","\n","        # 이미지와 마스크를 반환합니다.\n","        return image, mask"],"metadata":{"id":"JbDkfkr9VeQD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n"],"metadata":{"id":"UQGkkaA-V9bA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 데이터 변환 파이프라인을 설정합니다. 이 파이프라인은 이미지에 대한 전처리 작업을 정의합니다.\n","#albumentations를 A로 두고 transform사용 -> albumentations는 torchvision보다 더 다양하고 빠른 모델임..\n","transform = A.Compose(\n","    [\n","        # 이미지를 224x224 크기로 조정합니다.\n","        A.Resize(224, 224),\n","\n","        # 이미지를 정규화합니다. (평균과 표준편차를 사용하여 스케일링)\n","        A.Normalize(),\n","\n","        # 이미지를 PyTorch 텐서로 변환합니다.\n","        ToTensorV2()\n","    ]\n",")\n"],"metadata":{"id":"_s_scz7BVvOc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = CustomDataset(csv_file='train_source.csv', transform=transform)\n"],"metadata":{"id":"zaOQ4yg1WCib"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from PIL import Image\n","import matplotlib.pyplot as plt\n"],"metadata":{"id":"86EotNXCYNy5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지를 읽어옵니다.\n","example = cv2.imread(\"/content/drive/MyDrive/open/train_source_image/TRAIN_SOURCE_0000.png\")\n","\n","# OpenCV는 이미지를 BGR 형식으로 읽어오기 때문에 RGB로 변환해줍니다.\n","example = cv2.cvtColor(example, cv2.COLOR_BGR2RGB)\n","\n","# 이미지를 출력합니다.\n","plt.imshow(example)\n","plt.axis('off')  # 축을 숨깁니다.\n","plt.show()"],"metadata":{"id":"WLtTCMv-WGC_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segmentation_map = cv2.imread (\"/content/drive/MyDrive/open/train_source_gt/TRAIN_SOURCE_0000.png\")\n","\n","\n","segmentation_map"],"metadata":{"id":"Ewd8yz5eWOsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["np.unique(segmentation_map)"],"metadata":{"id":"I7j0p-50Y6It"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def color_palette():\n","    \"\"\"Color palette that maps each class to RGB values.\n","\n","    This one is actually taken from ADE20k.\n","    \"\"\"\n","    return [\n","    [0, 0, 0],     # 검정\n","    [255, 255, 255], # 흰색\n","    [255, 0, 0],   # 빨강\n","    [0, 255, 0],   # 초록\n","    [0, 0, 255],   # 파랑\n","    [255, 255, 0], # 노랑\n","    [255, 0, 255], # 마젠타\n","    [0, 255, 255], # 시안\n","    [128, 128, 128], # 회색\n","    [128, 0, 0],   # 진한 빨강\n","    [0, 128, 0],   # 진한 초록\n","    [0, 0, 128],   # 진한 파랑\n","    ]\n","\n","palette = color_palette()"],"metadata":{"id":"usfQZhlcZAjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 이미지를 2D 형태로 변환\n","segmentation_map = cv2.imread(\"/content/drive/MyDrive/open/train_source_gt/TRAIN_SOURCE_0000.png\", cv2.IMREAD_GRAYSCALE)\n","\n","# 또는 다음과 같이 할 수도 있습니다.\n","# segmentation_map = cv2.imread(\"/content/drive/MyDrive/open/train_source_gt/TRAIN_SOURCE_0000.png\")\n","# segmentation_map = cv2.cvtColor(segmentation_map, cv2.COLOR_BGR2GRAY)\n","\n","segmentation_map = np.array(segmentation_map)"],"metadata":{"id":"UD1Vj6_4jhxw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["segmentation_map"],"metadata":{"id":"0_zKtXUoplGF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels = {\n","    0: [6,7],\n","    1: [8],\n","    2: [11,12,13,15],\n","    3: [17,18,14],\n","    4: [1],\n","    5: [19],\n","    6: [20],\n","    7: [21],\n","    8: [23],\n","    9: [24,25],\n","    10: [],\n","    11: [26,27,28,29,30,32,33]\n","}"],"metadata":{"id":"BuZVhjF6bVwp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","image = example\n","color_segmentation_map = np.zeros((segmentation_map.shape[0], segmentation_map.shape[1], 3), dtype=np.uint8) # height, width, 3\n","for label, color in enumerate(palette):\n","    color_segmentation_map[segmentation_map - 1 == label, :] = color\n","# Convert to BGR\n","ground_truth_color_seg = color_segmentation_map[..., ::-1]\n","\n","img = np.array(image) * 0.5 + ground_truth_color_seg * 0.5\n","img = img.astype(np.uint8)\n","\n","plt.figure(figsize=(15, 10))\n","plt.imshow(img)\n","plt.show()"],"metadata":{"id":"o1k0KOEMbL-f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_ds = {\n","    'train': {\n","        'pixel_values': image,  # 여기서 my_image는 단일 이미지 데이터입니다.\n","        'label': segmentation_map,  # 여기서 my_label은 해당 이미지의 라벨입니다.\n","    },\n","    'num_rows': 1  # 단일 이미지이므로 num_rows는 1입니다.\n","}\n","test_image = cv2.imread(\"/content/drive/MyDrive/open/test_image/TEST_0000.png\")\n","\n","test_ds = {\n","    'test': {\n","        'pixel_values': image,  # 여기서 my_image는 단일 이미지 데이터입니다.\n","        'label': segmentation_map,  # 여기서 my_label은 해당 이미지의 라벨입니다.\n","    },\n","    'num_rows': 1  # 단일 이미지이므로 num_rows는 1입니다.\n","}"],"metadata":{"id":"uGGeJ5Sqs_Rh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","from torch.utils.data import Dataset\n","\n","class ImageSegmentationDataset(Dataset):\n","    \"\"\"Image segmentation dataset.\"\"\"\n","\n","    def __init__(self, dataset, transform):\n","        \"\"\"\n","        Args:\n","            dataset\n","        \"\"\"\n","        self.dataset = dataset\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.dataset)\n","\n","    def __getitem__(self, idx):\n","        original_image = np.array(self.dataset[idx]['pixel_values'])\n","        original_segmentation_map = np.array(self.dataset[idx]['label'])\n","\n","        transformed = self.transform(image=original_image, mask=original_segmentation_map)\n","        image, segmentation_map = transformed['image'], transformed['mask']\n","\n","        # convert to C, H, W\n","        image = image.transpose(2,0,1)\n","\n","        return image, segmentation_map, original_image, original_segmentation_map"],"metadata":{"id":"mOsVoCn7r3Lz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import albumentations as A\n","\n","ADE_MEAN = np.array([123.675, 116.280, 103.530]) / 255\n","ADE_STD = np.array([58.395, 57.120, 57.375]) / 255\n","\n","train_transform = A.Compose([\n","    A.LongestMaxSize(max_size=1333),\n","    A.RandomCrop(width=512, height=512),\n","    A.HorizontalFlip(p=0.5),\n","    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n","])\n","\n","test_transform = A.Compose([\n","    A.Resize(width=512, height=512),\n","    A.Normalize(mean=ADE_MEAN, std=ADE_STD),\n","\n","])\n","\n","train_dataset = ImageSegmentationDataset(train_ds, transform=train_transform)\n","# test_dataset = ImageSegmentationDataset(test_ds, transform=test_transform)"],"metadata":{"id":"nHLq3NHjurgH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_dataset)\n"],"metadata":{"id":"IWvosHO9w7jf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image, segmentation_map, _, _ = train_dataset[0]\n","print(image.shape)\n","print(segmentation_map.shape)"],"metadata":{"id":"eUXn_2qLv1DI"},"execution_count":null,"outputs":[]}]}